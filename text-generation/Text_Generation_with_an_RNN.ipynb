{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation with an RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourcecode369/100-days-of-ml-code/blob/master/text-generation/Text_Generation_with_an_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlXYTQSmKsdf",
        "colab_type": "code",
        "outputId": "6d6a9a1d-81ea-4235-9cdb-fffc3711ba24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, print_function, unicode_literals, absolute_import\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfDV-_zRq1Zi",
        "colab_type": "code",
        "outputId": "b198c377-020e-4a6f-9de3-aff44cd9c04f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JauRNGVaz8kJ",
        "colab_type": "code",
        "outputId": "5f576018-a128-4bef-c522-9b56c299ac33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFckfKAYVt9A",
        "colab_type": "code",
        "outputId": "aff6d19f-1043-463c-fa2c-a3c06857cc6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43IIQKzOVzj9",
        "colab_type": "code",
        "outputId": "0f6460ea-e028-4945-c808-a3adf75657a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print('{} unique characters.'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62J12YuBV9Te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmukF-UTWrvl",
        "colab_type": "code",
        "outputId": "540cc682-278e-43ca-9d65-93acffe43658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print(\"{\")\n",
        "for char, _ in zip(char2idx, range(20)):\n",
        "  print(' {:4s}: {:3d}, '.format(repr(char), char2idx[char]))\n",
        "print('   ...\\n')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            " '\\n':   0, \n",
            " ' ' :   1, \n",
            " '!' :   2, \n",
            " '$' :   3, \n",
            " '&' :   4, \n",
            " \"'\" :   5, \n",
            " ',' :   6, \n",
            " '-' :   7, \n",
            " '.' :   8, \n",
            " '3' :   9, \n",
            " ':' :  10, \n",
            " ';' :  11, \n",
            " '?' :  12, \n",
            " 'A' :  13, \n",
            " 'B' :  14, \n",
            " 'C' :  15, \n",
            " 'D' :  16, \n",
            " 'E' :  17, \n",
            " 'F' :  18, \n",
            " 'G' :  19, \n",
            "   ...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-LXjO_b_ESz",
        "colab_type": "code",
        "outputId": "3040c136-6bd0-4851-e4e2-8df41ed9b983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f'{repr(text[:15])} ------ characters mapped to int ------------> {text_as_int[:15]}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\n' ------ characters mapped to int ------------> [18 47 56 57 58  1 15 47 58 47 64 43 52 10  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQFj9Vmo_USn",
        "colab_type": "text"
      },
      "source": [
        "### The prediction task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITCQD0wp_f1c",
        "colab_type": "text"
      },
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task we're training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the outputâ€”the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt-0U_e5_jXE",
        "colab_type": "text"
      },
      "source": [
        "### Create training examples and targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcsPhghp_oLu",
        "colab_type": "text"
      },
      "source": [
        "Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQfZbeLfAGZI",
        "colab_type": "code",
        "outputId": "5ce97214-b421-4f21-c7b1-f2e4ab7c6a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "seq_length = 100\n",
        "\n",
        "examples_per_epoch = len(text)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(10):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j04PcZqgAYkY",
        "colab_type": "code",
        "outputId": "b43546f6-540a-4f6f-c29f-cf3c69469d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(''.join(idx2char[item.numpy()]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n",
            "now Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us ki\n",
            "ll him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be d\n",
            "one: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeXx-LmUAqYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXg8DkEuBEqP",
        "colab_type": "code",
        "outputId": "d62107db-c6e8-4e98-c972-30747e76ee79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print('Target data: ',repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaqDTlWRBdzR",
        "colab_type": "code",
        "outputId": "6651606c-7bb6-4478-cede-04d774996e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "  print(\"Step: {:4d}\".format(i))\n",
        "  print(\" input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "  print(\" target: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:    0\n",
            " input: 18 ('F')\n",
            " target: 47 ('i')\n",
            "Step:    1\n",
            " input: 47 ('i')\n",
            " target: 56 ('r')\n",
            "Step:    2\n",
            " input: 56 ('r')\n",
            " target: 57 ('s')\n",
            "Step:    3\n",
            " input: 57 ('s')\n",
            " target: 58 ('t')\n",
            "Step:    4\n",
            " input: 58 ('t')\n",
            " target: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMGIMhFnCDr5",
        "colab_type": "code",
        "outputId": "a623df5e-c68b-40a1-be81-01f2d20a2866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset.element_spec"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KujPjQ1WHbcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KscWxsdHf6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "                               tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')),\n",
        "                               tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')),\n",
        "                               tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRGd58psIeGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size=len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDP0NUPSIoBg",
        "colab_type": "code",
        "outputId": "fdd4a502-dfa0-44ff-cdba-a500f1fda2db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (64, None, 2048)          7876608   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (64, None, 2048)          18886656  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            133185    \n",
            "=================================================================\n",
            "Total params: 26,913,089\n",
            "Trainable params: 26,913,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGdTN1rhIqG0",
        "colab_type": "code",
        "outputId": "5047a8f8-5a77-4cc4-c4e5-929ae9b42ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# batch_size, sequence_length, vocab_size\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # batch_size, sequence_length, vocab_size\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PjS4sucJGWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npj-JSkfVaQb",
        "colab_type": "text"
      },
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "**Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop.**c\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh3vQlGJUXRa",
        "colab_type": "code",
        "outputId": "94596a76-5917-414e-a1bc-8cda11ab8fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Input: \\n', repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\",repr(\"\".join(idx2char[sampled_indices])))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'g of catching nature,\\nSpread further.\\n\\nMENENIUS:\\nOne word more, one word.\\nThis tiger-footed rage, wh'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"yKRniERe\\nRVdDIA-aN;ngk?vqlPhddkc'xgIlChlMcRNNkKWXD'bfWVu?lkb!Tt;rxUqE$MTVDryk3;GP\\nDZwS:O;.ClrV&NJGYW\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4u_oM0AVEhJ",
        "colab_type": "code",
        "outputId": "bc160c97-06a2-48d7-efaa-33d7508fdcff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Predictions shape: \", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Scalar loss: \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions shape:  (64, 100, 65) # (batch_size, sequence_length, vocab_size)\n",
            "Scalar loss:  4.1730747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3ryOgaCWnnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqYXaVi1XMx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mti4--7oXTE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXLZsclGXVfs",
        "colab_type": "code",
        "outputId": "efade437-163e-4efa-a70a-84eb2e39eb5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 172 steps\n",
            "Epoch 1/20\n",
            "172/172 [==============================] - 41s 236ms/step - loss: 0.8778\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 37s 217ms/step - loss: 0.0689\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 37s 217ms/step - loss: 0.0546\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 37s 218ms/step - loss: 0.0444\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 37s 217ms/step - loss: 0.0372\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 37s 218ms/step - loss: 0.0328\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 37s 218ms/step - loss: 0.0305\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 37s 218ms/step - loss: 0.0283\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 38s 219ms/step - loss: 0.0249\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 38s 218ms/step - loss: 0.0229\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 38s 218ms/step - loss: 0.0211\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 38s 219ms/step - loss: 0.0194\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 38s 219ms/step - loss: 0.0174\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 38s 219ms/step - loss: 0.0160\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 38s 220ms/step - loss: 0.0144\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 38s 219ms/step - loss: 0.0127\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 38s 219ms/step - loss: 0.0116\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 38s 219ms/step - loss: 0.0102\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 38s 220ms/step - loss: 0.0091\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 38s 220ms/step - loss: 0.0082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_VG7FKHXc3N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab5cfcf8-6343-438d-bb99-f90d6b1693e8"
      },
      "source": [
        "# https://github.com/Skuldur/Classical-Piano-Composer\n",
        "\n",
        "tf.train.latest_checkpoint(checkpoint_dir=checkpoint_dir)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_20'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMD4PakLCe3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir=checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-PcdxWgC5dX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "c2bff6d8-68cf-44de-9be1-d96aafc0fa7c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (1, None, 2048)           7876608   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (1, None, 2048)           18886656  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             133185    \n",
            "=================================================================\n",
            "Total params: 26,913,089\n",
            "Trainable params: 26,913,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zq22ktRDCwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_generate = 1000\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}